import os
import mimetypes
import asyncio
from io import BytesIO

import pandas as pd
from pathlib import Path
from fastapi import APIRouter, Depends, HTTPException, UploadFile, Request, status, Query
from starlette.datastructures import UploadFile as StarletteUploadFile
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from starlette.responses import StreamingResponse

from app.core.database import SessionLocal
from app.models.dataset import Dataset
from app.schemas.dataset_schema import DatasetRead, DatasetListResponse
from app.crud.dataset_crud import DatasetCRUD
from app.utils.pressure_test_util import compute_md5_bytes, upload_to_tos
from app.core.config import settings
from loguru import logger

router = APIRouter(prefix="/datasets", tags=["Datasets"])


# âœ… è·å–æ•°æ®åº“ä¼šè¯
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# âœ… ä¸Šä¼ æ•°æ®é›†
@router.post("/upload", response_model=DatasetRead, status_code=status.HTTP_201_CREATED)
async def upload_dataset(request: Request, db: Session = Depends(get_db)):
    """
    ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶åˆ°ç«å±± TOSï¼Œè®¡ç®— MD5 å¹¶å†™å…¥æ•°æ®åº“ã€‚
    è‹¥ç›¸åŒ MD5 æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥å¤ç”¨ã€‚
    """
    form = await request.form()
    upload = form.get("file")
    uploaded_by = form.get("uploaded_by", "anonymous")

    if not isinstance(upload, StarletteUploadFile):
        raise HTTPException(status_code=400, detail="å¿…é¡»åŒ…å« file å­—æ®µ")

    # è¯»å–æ–‡ä»¶å†…å®¹
    file_bytes = await upload.read()
    suffix = Path(upload.filename).suffix.lower()
    file_md5 = compute_md5_bytes(file_bytes)
    logger.info(f"ğŸ“¦ ä¸Šä¼ æ–‡ä»¶ {upload.filename} çš„ MD5: {file_md5}")

    # âœ… æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰ç›¸åŒæ•°æ®é›†
    existing = DatasetCRUD.get_by_md5(db, file_md5)
    if existing:
        logger.info(f"âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¤ç”¨æ•°æ®é›†: {existing.tos_url}")
        return existing

    # âœ… å†™å…¥ä¸´æ—¶æ–‡ä»¶
    upload_dir = Path(settings.FILE_UPLOAD_DIR)
    upload_dir.mkdir(parents=True, exist_ok=True)
    tmp_path = upload_dir / f"{file_md5}{suffix}"
    tmp_path.write_bytes(file_bytes)
    logger.info(f"ğŸ“„ å·²å†™å…¥ä¸´æ—¶æ–‡ä»¶: {tmp_path}")

    # âœ… ä¸Šä¼ åˆ° TOS
    tos_object_key = f"datasets/{file_md5}{suffix}"
    tos_url = None
    try:
        tos_url = await asyncio.to_thread(upload_to_tos, tmp_path, tos_object_key)
        logger.info(f"âœ… ä¸Šä¼ è‡³ç«å±± TOS æˆåŠŸ: {tos_url}")
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ è‡³ç«å±± TOS å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"TOS ä¸Šä¼ å¤±è´¥: {e}")

    # âœ… é¢„è§ˆæ–‡ä»¶å‰ 3 è¡Œï¼ˆåœ¨åˆ é™¤æ–‡ä»¶å‰æ‰§è¡Œï¼‰
    preview_rows = []
    try:
        if suffix == ".csv":
            df = pd.read_csv(tmp_path)
        elif suffix in [".xls", ".xlsx"]:
            df = pd.read_excel(tmp_path)
        preview_rows = df.head(3).to_dict(orient="records")
        logger.info(f"âœ… æ–‡ä»¶ {upload.filename} å‰ 3 è¡Œ: {preview_rows}")
    except Exception as e:
        logger.warning(f"âš ï¸ é¢„è§ˆæ–‡ä»¶å†…å®¹å¤±è´¥ï¼ˆéå…³é”®æ­¥éª¤ï¼‰{e}")

    # âœ… ä¸Šä¼ æˆåŠŸåå†åˆ é™¤æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
    try:
        if tmp_path.exists():
            tmp_path.unlink()
            logger.info(f"ğŸ§¹ å·²åˆ é™¤æœ¬åœ°ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
    except Exception as e:
        logger.warning(f"âš ï¸ åˆ é™¤æœ¬åœ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    # âœ… å†™å…¥æ•°æ®åº“
    try:
        from app.schemas.dataset_schema import DatasetCreate

        dataset_data = DatasetCreate(
            filename=upload.filename,
            file_md5=file_md5,
            file_suffix=suffix,
            tos_key=tos_object_key,
            tos_url=tos_url,
            preview_rows=preview_rows,
            uploaded_by=uploaded_by,
        )
        created = DatasetCRUD.create(db, dataset_data)
        logger.info(f"âœ… å·²ä¿å­˜æ•°æ®é›†è®°å½•: {created.uuid}")
    except SQLAlchemyError as e:
        logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="æ•°æ®åº“å†™å…¥å¤±è´¥")

    return created

# âœ… è·å–æ•°æ®é›†åˆ—è¡¨
@router.get("/list", response_model=DatasetListResponse)
def list_datasets(
    uploaded_by: str = Query(None, description="æŒ‰ä¸Šä¼ è€…è¿‡æ»¤"),
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    limit: int = Query(50, ge=1, le=500, description="æ¯é¡µæ•°é‡"),
    db: Session = Depends(get_db),
):
    """åˆ†é¡µåˆ—å‡ºä¸Šä¼ çš„æ•°æ®é›†"""
    # è®¡ç®—åç§»é‡
    offset = (page - 1) * limit

    # è·å–æ€»æ•°
    total = DatasetCRUD.count(db, uploaded_by=uploaded_by)

    # è·å–åˆ†é¡µæ•°æ®
    datasets = DatasetCRUD.list_all(db, uploaded_by=uploaded_by, limit=limit, offset=offset)

    return {
        "total": total,
        "page": page,
        "limit": limit,
        "items": [d.to_dict(exclude_none=True) for d in datasets],
    }


# âœ… è·å–å•ä¸ªæ•°æ®é›†è¯¦æƒ…
@router.get("/{uuid}", response_model=DatasetRead)
def get_dataset(uuid: str, db: Session = Depends(get_db)):
    """æ ¹æ® UUID è·å–å•ä¸ªæ•°æ®é›†è¯¦æƒ…"""
    dataset = DatasetCRUD.get_by_uuid(db, uuid)
    if not dataset:
        raise HTTPException(status_code=404, detail="æ•°æ®é›†ä¸å­˜åœ¨")
    return dataset


# âœ… åˆ é™¤æ•°æ®é›†ï¼ˆé€»è¾‘åˆ é™¤ï¼‰
@router.delete("/{uuid}", response_model=dict)
def delete_dataset(uuid: str, db: Session = Depends(get_db)):
    """é€»è¾‘åˆ é™¤æ•°æ®é›†"""
    success = DatasetCRUD.soft_delete(db, uuid)
    if not success:
        raise HTTPException(status_code=500, detail="åˆ é™¤å¤±è´¥")
    return {"message": f"âœ… æ•°æ®é›† {uuid} å·²æ ‡è®°ä¸ºåˆ é™¤"}

@router.get("/download/{dataset_uuid}")
def download_dataset(dataset_uuid: str, db: Session = Depends(get_db)):
    """
    âœ… æ ¹æ®æ•°æ®é›† UUID ä¸‹è½½æ–‡ä»¶
    1. æŸ¥è¯¢æ•°æ®åº“ â†’ è·å– file_md5, tos_key
    2. ä» TOS ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜
    3. FastAPI ä»¥æµå½¢å¼è¿”å›ä¸‹è½½
    """
    # 1ï¸âƒ£ æŸ¥æ‰¾æ•°æ®é›†
    dataset = db.query(Dataset).filter(
        Dataset.uuid == dataset_uuid,
        Dataset.is_deleted.is_(False)
    ).first()

    if not dataset:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°å¯¹åº”çš„æ•°æ®é›†")

    file_md5 = dataset.file_md5
    object_key = dataset.tos_key
    file_suffix = dataset.file_suffix or ".csv"
    filename = dataset.filename or f"{file_md5}{file_suffix}"

    logger.info(f"ğŸ“¦ æ­£åœ¨ä¸‹è½½æ•°æ®é›†: uuid={dataset_uuid}, md5={file_md5}, key={object_key}")

    # 2ï¸âƒ£ ä¸‹è½½æ–‡ä»¶åˆ°å†…å­˜
    try:
        import tos
        import os

        ak = os.getenv("TOS_ACCESS_KEY")
        sk = os.getenv("TOS_SECRET_KEY")
        endpoint = os.getenv("TOS_ENDPOINT")
        region = os.getenv("TOS_REGION")
        bucket_name = os.getenv("TOS_BUCKET")

        client = tos.TosClientV2(ak, sk, endpoint, region)
        obj_stream = client.get_object(bucket_name, object_key.__str__())
        buffer = BytesIO()

        for chunk in obj_stream:
            buffer.write(chunk)
        buffer.seek(0)

        logger.success(f"âœ… æˆåŠŸä¸‹è½½è‡³å†…å­˜: {filename}")

    except Exception as e:
        logger.error(f"âŒ ä» TOS ä¸‹è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"TOS ä¸‹è½½å¤±è´¥: {e}")

    # 3ï¸âƒ£ è¿”å› StreamingResponseï¼Œè®©æµè§ˆå™¨ä¸‹è½½
    media_type, _ = mimetypes.guess_type(filename)
    media_type = media_type or "application/octet-stream"

    headers = {
        "Content-Disposition": f'attachment; filename="{filename}"',
        "Content-Type": media_type,
    }

    return StreamingResponse(buffer, headers=headers, media_type=media_type)
