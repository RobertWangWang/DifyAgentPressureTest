import os
import mimetypes
import asyncio
from io import BytesIO
from pathlib import Path

import pandas as pd
from fastapi import (
    APIRouter, Depends, HTTPException, Request, status, Query
)
from starlette.datastructures import UploadFile as StarletteUploadFile
from starlette.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError
from loguru import logger

from app.core.database import SessionLocal
from app.models.dataset import Dataset
from app.schemas.dataset_schema import DatasetRead, DatasetListResponse, DatasetCreate
from app.crud.dataset_crud import DatasetCRUD
from app.utils.pressure_test_util import compute_md5_bytes, upload_to_tos, dify_api_url_2_account_profile_url, \
    dify_get_account_id
from app.core.config import settings

router = APIRouter(prefix="/datasets", tags=["Datasets"])


# âœ… è·å–æ•°æ®åº“ä¼šè¯
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# âœ… ä¸Šä¼ æ•°æ®é›†
@router.post("/upload", response_model=DatasetRead, status_code=status.HTTP_201_CREATED)
async def upload_dataset(request: Request, db: Session = Depends(get_db)):
    """
    ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶åˆ°ç«å±± TOSï¼Œè®¡ç®— MD5 å¹¶å†™å…¥æ•°æ®åº“ã€‚
    è‹¥ (uploaded_by, file_md5, agent_id) å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥å¤ç”¨ã€‚
    """
    form = await request.form()
    upload = form.get("file")
    uploaded_by = form.get("uploaded_by", "anonymous")
    agent_id = form.get("agent_id")

    account_profile_url = dify_api_url_2_account_profile_url(form.get("dify_api_url"))
    dify_account_id = dify_get_account_id(account_profile_url, uploaded_by)

    if not agent_id:
        raise HTTPException(status_code=400, detail="å¿…é¡»æä¾› agent_id")
    if not isinstance(upload, StarletteUploadFile):
        raise HTTPException(status_code=400, detail="å¿…é¡»åŒ…å« file å­—æ®µ")

    # è¯»å–æ–‡ä»¶
    file_bytes = await upload.read()
    suffix = Path(upload.filename).suffix.lower()
    file_md5 = compute_md5_bytes(file_bytes)
    logger.info(f"ğŸ“¦ ä¸Šä¼ æ–‡ä»¶ {upload.filename} çš„ MD5: {file_md5}")

    # âœ… å»é‡é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶
    # existing = DatasetCRUD.get_by_md5(db, file_md5, agent_id=agent_id, uploaded_by=dify_account_id)
    # logger.debug(f"âœ… æ£€æŸ¥æ•°æ®é›†: {existing}")
    # if existing:
    #     DatasetCRUD.update_by_uuid(db, existing.uuid, **{"is_deleted":False})
    #     logger.info(f"âœ… æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¤ç”¨æ•°æ®é›†: {existing.tos_url}")
    #     return existing
    # elif not existing:
    #     pass

    # âœ… å†™å…¥ä¸´æ—¶æ–‡ä»¶
    upload_dir = Path(settings.FILE_UPLOAD_DIR)
    upload_dir.mkdir(parents=True, exist_ok=True)
    tmp_path = upload_dir / f"{file_md5}{suffix}"
    tmp_path.write_bytes(file_bytes)

    # âœ… ä¸Šä¼ åˆ° TOS
    tos_object_key = f"datasets/{agent_id}/{file_md5}{suffix}"
    try:
        tos_url = await asyncio.to_thread(upload_to_tos, tmp_path, tos_object_key)
    except Exception as e:
        logger.exception("âŒ ä¸Šä¼ è‡³ TOS å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"TOS ä¸Šä¼ å¤±è´¥: {e}")
    finally:
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if tmp_path.exists():
            tmp_path.unlink(missing_ok=True)

    # âœ… é¢„è§ˆå‰3è¡Œ
    preview_rows = []
    try:
        df = None
        if suffix == ".csv":
            df = pd.read_csv(BytesIO(file_bytes))
        elif suffix in [".xls", ".xlsx"]:
            df = pd.read_excel(BytesIO(file_bytes))
        preview_rows = df.head(3).to_dict(orient="records")
    except Exception as e:
        logger.warning(f"âš ï¸ é¢„è§ˆæ–‡ä»¶å¤±è´¥: {e}")

    # âœ… å†™å…¥æ•°æ®åº“
    dataset_data = DatasetCreate(
        filename=upload.filename,
        file_md5=file_md5,
        file_suffix=suffix,
        tos_key=tos_object_key,
        tos_url=tos_url,
        preview_rows=preview_rows,
        uploaded_by=dify_account_id,
        agent_id=agent_id,
    )

    try:
        created = DatasetCRUD.create(db, dataset_data)
        logger.success(f"âœ… å·²ä¿å­˜æ•°æ®é›†è®°å½•: {created.uuid}")
        return created
    except SQLAlchemyError as e:
        logger.exception("âŒ æ•°æ®åº“å†™å…¥å¤±è´¥")
        raise HTTPException(status_code=500, detail="æ•°æ®åº“å†™å…¥å¤±è´¥")


# âœ… åˆ†é¡µæŸ¥è¯¢æ•°æ®é›†
@router.get("/list", response_model=DatasetListResponse)
def list_datasets(
    uploaded_by: str = Query(None, description="æŒ‰ä¸Šä¼ è€…è¿‡æ»¤"),
    agent_id: str = Query(None, description="æŒ‰ agent_id è¿‡æ»¤"),
    dify_api_url: str = Query(..., description="Dify API URL"),
    page: int = Query(1, ge=1, description="é¡µç ï¼Œä»1å¼€å§‹"),
    limit: int = Query(50, ge=1, le=500, description="æ¯é¡µæ•°é‡"),
    db: Session = Depends(get_db),
):
    logger.debug(f"åˆ†é¡µæŸ¥è¯¢æ•°æ®é›†: uploaded_by={uploaded_by}, agent_id={agent_id}, dify_api_url={dify_api_url}")
    account_profile_url = dify_api_url_2_account_profile_url(dify_api_url)
    dify_account_id = dify_get_account_id(account_profile_url,uploaded_by)
    """åˆ†é¡µåˆ—å‡ºä¸Šä¼ çš„æ•°æ®é›†"""
    offset = (page - 1) * limit
    total = DatasetCRUD.count(db, uploaded_by=dify_account_id, agent_id=agent_id)
    datasets = DatasetCRUD.list_all(db, uploaded_by=dify_account_id, agent_id=agent_id, limit=limit, offset=offset)

    return {"total": total, "page": page, "limit": limit, "items": [d.to_dict(exclude_none=True) for d in datasets]}


# âœ… è·å–å•ä¸ªæ•°æ®é›†è¯¦æƒ…
@router.get("/{uuid}", response_model=DatasetRead)
def get_dataset(uuid: str, db: Session = Depends(get_db)):
    dataset = DatasetCRUD.get_by_uuid(db, uuid)
    if not dataset:
        raise HTTPException(status_code=404, detail="æ•°æ®é›†ä¸å­˜åœ¨")
    return dataset


# âœ… åˆ é™¤æ•°æ®é›†ï¼ˆé€»è¾‘åˆ é™¤ï¼‰
@router.delete("/{uuid}", response_model=dict)
def delete_dataset(uuid: str, db: Session = Depends(get_db)):
    success = DatasetCRUD.soft_delete(db, uuid)
    if not success:
        raise HTTPException(status_code=500, detail="åˆ é™¤å¤±è´¥")
    return {"message": f"âœ… æ•°æ®é›† {uuid} å·²æ ‡è®°ä¸ºåˆ é™¤"}


# âœ… ä¸‹è½½æ•°æ®é›†æ–‡ä»¶
@router.get("/download/{dataset_uuid}")
def download_dataset(dataset_uuid: str, db: Session = Depends(get_db)):
    dataset = db.query(Dataset).filter(Dataset.uuid == dataset_uuid, Dataset.is_deleted.is_(False)).first()
    if not dataset:
        raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°å¯¹åº”çš„æ•°æ®é›†")

    try:
        import tos
        ak, sk = os.getenv("TOS_ACCESS_KEY"), os.getenv("TOS_SECRET_KEY")
        endpoint, region, bucket = os.getenv("TOS_ENDPOINT"), os.getenv("TOS_REGION"), os.getenv("TOS_BUCKET")
        client = tos.TosClientV2(ak, sk, endpoint, region)
        obj_stream = client.get_object(bucket, dataset.tos_key)
        buffer = BytesIO()
        for chunk in obj_stream:
            buffer.write(chunk)
        buffer.seek(0)
    except Exception as e:
        logger.exception("âŒ ä» TOS ä¸‹è½½å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"TOS ä¸‹è½½å¤±è´¥: {e}")

    filename = dataset.filename or f"{dataset.file_md5}{dataset.file_suffix or '.csv'}"
    media_type, _ = mimetypes.guess_type(filename)
    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    return StreamingResponse(buffer, headers=headers, media_type=media_type or "application/octet-stream")
