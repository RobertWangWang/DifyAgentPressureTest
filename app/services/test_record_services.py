from pathlib import Path
import pandas as pd
import numpy as np
import asyncio
import aiohttp
from fastapi import Request, HTTPException
from sqlalchemy.orm import Session
import os

from app.utils.pressure_test_util import (
    single_test_chatflow_non_stream_pressure,
    single_test_workflow_non_stream_pressure,
    validate_entry,
    get_agent_input_para_dict,
    download_from_tos
)
from app.core.database import SessionLocal
from app.utils.logger import logger
from app.models.test_record import TestRecord, TestStatus
from app.crud.test_record_crud import TestRecordCRUD
from app.crud.single_run_result_crud import SingleRunResultCRUD


# ==========================================================
# é€šç”¨å·¥å…·å‡½æ•°
# ==========================================================

def align_dify_input_types(df_data: pd.DataFrame, df_schema: pd.DataFrame) -> pd.DataFrame:
    """æ ¹æ® Dify å‚æ•° schema å¯¹ç”¨æˆ·ä¸Šä¼ çš„æ•°æ®è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚"""
    df_result = df_data.copy()
    for _, row in df_schema.iterrows():
        var = row["variable"]
        typ = row["type"]
        options = row["options"]
        required = row["required"]

        if var not in df_result.columns:
            logger.warning(f"âš ï¸ Missing column in data: {var}")
            df_result[var] = np.nan
            continue

        if typ in ("text-input", "paragraph"):
            df_result[var] = df_result[var].astype(str).fillna("")
        elif typ == "number":
            df_result[var] = pd.to_numeric(df_result[var], errors="coerce")
            if required and df_result[var].isna().any():
                logger.error(f"âŒ Required numeric field '{var}' has invalid values")
        elif typ == "select":
            df_result[var] = df_result[var].astype(str)
            valid_opts = set(options)
            if valid_opts:
                df_result[var] = df_result[var].apply(lambda x: x if x in valid_opts else None)
                if required and df_result[var].isna().any():
                    logger.warning(f"âš ï¸ Invalid choice in '{var}'")
        else:
            logger.info(f"â„¹ï¸ Unknown type {typ}, left unchanged")

    return df_result


# ==========================================================
# åŒæ­¥æ£€æµ‹å‡½æ•°ï¼ˆæ›¿ä»£å¼‚æ­¥æ£€æµ‹ï¼‰
# ==========================================================

def check_cancelled_sync(_, uuid_str: str):
    # ğŸ”¥ æ–°å»ºç‹¬ç«‹ sessionï¼Œé˜²æ­¢ç¼“å­˜æ±¡æŸ“
    with SessionLocal() as local_db:
        record = local_db.query(TestRecord.status).filter(TestRecord.uuid == uuid_str).first()
        if record and record[0] == TestStatus.CANCELLED:
            logger.warning(f"ğŸš« æ£€æµ‹åˆ°ä»»åŠ¡ {uuid_str} å·²è¢«å–æ¶ˆï¼Œä¸­æ­¢æ‰§è¡Œã€‚")
            raise asyncio.CancelledError()


# ==========================================================
# Chatflow å¼‚æ­¥æ‰§è¡Œå™¨ï¼ˆåŒæ­¥æ£€æµ‹å–æ¶ˆï¼‰
# ==========================================================

async def run_chatflow_tests_async(
    df,
    input_uuid: str,
    input_dify_url: str,
    input_dify_api_key: str,
    input_dify_username: str,
    input_judge_prompt: str,
    llm,
    db: Session,
    concurrency: int = 10,
):
    """å¼‚æ­¥é™å¹¶å‘æ‰§è¡Œ Chatflow æµ‹è¯•ä»»åŠ¡ï¼Œæ£€æµ‹åˆ°å–æ¶ˆæ—¶ç«‹å³ç»ˆæ­¢æ‰€æœ‰ä»»åŠ¡ã€‚"""
    semaphore = asyncio.Semaphore(concurrency)
    all_results = []

    async def _run_single(index, row, session):
        check_cancelled_sync(db, input_uuid)
        row_dict = row.to_dict()
        input_query = row_dict.get("query", "")
        async with semaphore:
            try:
                logger.debug(f"å¼€å§‹æ‰§è¡Œç¬¬ {index + 1} è¡Œ Chatflow æµ‹è¯•")
                result = await asyncio.to_thread(
                    single_test_chatflow_non_stream_pressure,
                    input_dify_url=input_dify_url,
                    input_dify_api_key=input_dify_api_key,
                    input_query=input_query,
                    input_dify_username=input_dify_username,
                    input_data_dict=row_dict,
                    input_judge_prompt=input_judge_prompt,
                    llm=llm,
                )

                check_cancelled_sync(db, input_uuid)
                await asyncio.to_thread(TestRecordCRUD.increment_success_count, input_uuid)
                single_run_data = {
                    "chatflow_query": input_query,
                    "test_params": row_dict,
                    "input_task_uuid": input_uuid,
                    "input_time_consumption": result["time_consumption"],
                    "input_score": result["score"],
                    "input_tps": result["TPS"],
                    "input_generated_answer": result["generated_answer"],
                }
                await asyncio.to_thread(SingleRunResultCRUD.create, **single_run_data)
                logger.success(f"âœ… [Row {index + 1}] æµ‹è¯•å®Œæˆ")
                return result

            except asyncio.CancelledError:
                logger.warning(f"âŒ [Row {index + 1}] Chatflow æµ‹è¯•è¢«å–æ¶ˆ")
                raise
            except Exception as e:
                await asyncio.to_thread(TestRecordCRUD.increment_failure_count, input_uuid)
                logger.error(f"âŒ [Row {index + 1}] Chatflow å‡ºé”™: {e}")
                return {"index": index, "error": str(e)}

    logger.info(f"ğŸš€ å¯åŠ¨ Chatflow å¼‚æ­¥æµ‹è¯•ï¼Œå…± {len(df)} æ¡è®°å½•ï¼Œæœ€å¤§å¹¶å‘={concurrency}")

    try:
        async with aiohttp.ClientSession() as session:
            tasks = [_run_single(i, row, session) for i, row in df.iterrows()]
            pending = set(asyncio.create_task(t) for t in tasks)

            while pending:
                done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)

                for task in done:
                    try:
                        check_cancelled_sync(db, input_uuid)
                        result = task.result()
                        all_results.append(result)
                    except asyncio.CancelledError:
                        logger.warning(f"ğŸš« æ£€æµ‹åˆ°ä»»åŠ¡ {input_uuid} è¢«å–æ¶ˆï¼Œå–æ¶ˆæ‰€æœ‰å‰©ä½™ä»»åŠ¡")
                        for p in pending:
                            p.cancel()
                        raise
                    except Exception as e:
                        logger.error(f"âŒ å­ä»»åŠ¡å¼‚å¸¸: {e}")

    except asyncio.CancelledError:
        logger.warning(f"ğŸ›‘ Chatflow æµ‹è¯•ä»»åŠ¡ {input_uuid} å·²ç»ˆæ­¢")
        raise

    logger.info(f"ğŸ Chatflow å¼‚æ­¥æµ‹è¯•å®Œæˆï¼Œå…± {len(all_results)} æ¡ç»“æœ")
    return all_results


# ==========================================================
# Workflow å¼‚æ­¥æ‰§è¡Œå™¨ï¼ˆåŒæ­¥æ£€æµ‹å–æ¶ˆï¼‰
# ==========================================================

async def run_workflow_tests_async(
    df,
    input_uuid: str,
    input_dify_url: str,
    input_dify_api_key: str,
    input_dify_username: str,
    input_judge_prompt: str,
    llm,
    db: Session,
    concurrency: int = 10,
):
    """å¼‚æ­¥é™å¹¶å‘æ‰§è¡Œ Workflow æµ‹è¯•ä»»åŠ¡ï¼Œæ£€æµ‹åˆ°å–æ¶ˆåç«‹å³ç»ˆæ­¢æ‰€æœ‰ä»»åŠ¡ã€‚"""
    semaphore = asyncio.Semaphore(concurrency)
    all_results = []

    async def _run_single(index, row, session):
        # æ¯ä¸ªå­ä»»åŠ¡å¼€å§‹å‰æ£€æµ‹
        check_cancelled_sync(db, input_uuid)
        row_dict = row.to_dict()

        async with semaphore:
            try:
                logger.debug(f"å¼€å§‹æ‰§è¡Œç¬¬ {index + 1} è¡Œ Workflow æµ‹è¯•")
                result = await asyncio.to_thread(
                    single_test_workflow_non_stream_pressure,
                    input_dify_url=input_dify_url,
                    input_dify_api_key=input_dify_api_key,
                    input_dify_username=input_dify_username,
                    input_data_dict=row_dict,
                    input_judge_prompt=input_judge_prompt,
                    llm=llm,
                )

                # æ¯ä¸ªä»»åŠ¡å®Œæˆåå†æ¬¡æ£€æµ‹å–æ¶ˆçŠ¶æ€
                check_cancelled_sync(db, input_uuid)

                await asyncio.to_thread(TestRecordCRUD.increment_success_count, input_uuid)

                single_run_data = {
                    "chatflow_query": "",
                    "test_params": row_dict,
                    "input_task_uuid": input_uuid,
                    "input_time_consumption": result["time_consumption"],
                    "input_score": result["score"],
                    "input_tps": result["TPS"],
                    "input_generated_answer": result["generated_answer"],
                }
                await asyncio.to_thread(SingleRunResultCRUD.create, **single_run_data)

                logger.success(f"âœ… [Row {index + 1}] Workflow æµ‹è¯•å®Œæˆ")
                return result

            except asyncio.CancelledError:
                logger.warning(f"âŒ [Row {index + 1}] Workflow æµ‹è¯•è¢«å–æ¶ˆ")
                raise

            except Exception as e:
                await asyncio.to_thread(TestRecordCRUD.increment_failure_count, input_uuid)
                logger.error(f"âŒ [Row {index + 1}] Workflow å‡ºé”™: {e}")
                return {"index": index, "error": str(e)}

    logger.info(f"ğŸš€ å¯åŠ¨ Workflow å¼‚æ­¥æµ‹è¯•ï¼Œå…± {len(df)} æ¡è®°å½•ï¼Œæœ€å¤§å¹¶å‘={concurrency}")

    try:
        async with aiohttp.ClientSession() as session:
            # âœ… åˆ›å»ºæ‰€æœ‰ä»»åŠ¡å¹¶æ”¾å…¥ pending é›†åˆ
            tasks = [_run_single(i, row, session) for i, row in df.iterrows()]
            pending = set(asyncio.create_task(t) for t in tasks)

            while pending:
                # âœ… æ¯æ¬¡ç­‰å¾…æœ€å…ˆå®Œæˆçš„ä»»åŠ¡
                done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)

                for task in done:
                    try:
                        # ğŸ”¥ æ£€æµ‹æ˜¯å¦å–æ¶ˆï¼ˆåŒæ­¥ DB æ£€æŸ¥ï¼‰
                        check_cancelled_sync(db, input_uuid)
                        result = task.result()
                        all_results.append(result)
                    except asyncio.CancelledError:
                        logger.warning(f"ğŸš« æ£€æµ‹åˆ°ä»»åŠ¡ {input_uuid} å·²è¢«å–æ¶ˆï¼Œç»ˆæ­¢æ‰€æœ‰æœªå®Œæˆ Workflow ä»»åŠ¡")
                        # âœ… å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„åç¨‹ä»»åŠ¡
                        for p in pending:
                            if not p.done():
                                p.cancel()
                        # âœ… ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®‰å…¨é€€å‡º
                        await asyncio.gather(*pending, return_exceptions=True)
                        raise
                    except Exception as e:
                        logger.error(f"âŒ å­ä»»åŠ¡å¼‚å¸¸: {e}")

    except asyncio.CancelledError:
        logger.warning(f"ğŸ›‘ Workflow æµ‹è¯•ä»»åŠ¡ {input_uuid} è¢«ç”¨æˆ·å–æ¶ˆï¼Œä¸­æ­¢æ‰§è¡Œ")
        raise

    logger.info(f"ğŸ Workflow å¼‚æ­¥æµ‹è¯•å®Œæˆï¼Œå…± {len(all_results)} æ¡ç»“æœ")
    return all_results


# ==========================================================
# Wrapper å±‚ï¼ˆä¿æŒé€»è¾‘ä¸€è‡´ï¼‰
# ==========================================================

async def test_chatflow_non_stream_pressure_wrapper(
    testrecord,
    request,
    db,
    mode: str,
):
    """
    Chatflow å‹æµ‹ä»»åŠ¡åŒ…è£…å™¨ï¼ˆä» TOS ä¸‹è½½æ•°æ®é›†ã€æ‰§è¡Œåæ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼‰
    """

    # åŸºæœ¬å‚æ•°
    input_dify_url = testrecord.dify_api_url
    input_dify_api_key = testrecord.dify_api_key
    input_username = testrecord.dify_username
    input_concurrency = testrecord.concurrency
    input_judge_prompt = testrecord.judge_prompt
    dataset_key = testrecord.dataset_tos_key

    if not dataset_key:
        raise ValueError("âŒ å½“å‰è®°å½•ç¼ºå°‘ dataset_tos_keyï¼Œæ— æ³•ä» TOS ä¸‹è½½æ•°æ®é›†ã€‚")

    # 1ï¸âƒ£ ä¸‹è½½æ•°æ®é›†åˆ°ä¸´æ—¶æ–‡ä»¶
    tmp_dir = Path("uploads/tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    tmp_path = tmp_dir / f"{testrecord.uuid}_{os.path.basename(dataset_key)}"

    try:
        logger.info(f"â¬‡ï¸ æ­£åœ¨ä» TOS ä¸‹è½½æ•°æ®é›†: {dataset_key} â†’ {tmp_path}")
        await asyncio.to_thread(download_from_tos, dataset_key, tmp_path)
        logger.success(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ: {tmp_path}")
    except Exception as e:
        logger.error(f"âŒ ä» TOS ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError(f"ä¸‹è½½ TOS æ•°æ®é›†å¤±è´¥: {e}")

    # 2ï¸âƒ£ è¯»å–æ–‡ä»¶å†…å®¹
    try:
        suffix = tmp_path.suffix.lower()
        if suffix == ".csv":
            df = await asyncio.to_thread(pd.read_csv, tmp_path)
        elif suffix in [".xls", ".xlsx"]:
            df = await asyncio.to_thread(pd.read_excel, tmp_path, engine="openpyxl")
        else:
            raise ValueError(f"Unsupported file type: {suffix}")

        if mode == "experiment":
            df = df.head(3)

        logger.info(f"âœ… æ•°æ®é›† {tmp_path.name} è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œ")
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        raise HTTPException(status_code=400, detail=f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {e}")

    # 3ï¸âƒ£ éªŒè¯è¾“å…¥
    para_df = await asyncio.to_thread(get_agent_input_para_dict, input_dify_url, input_dify_api_key)
    df = align_dify_input_types(df, para_df)
    for _, row in df.iterrows():
        row_dict = row.to_dict()
        error = validate_entry(row_dict, para_df)
        if error:
            raise ValueError(f"è¾“å…¥éªŒè¯å¤±è´¥: {error}")

    # 4ï¸âƒ£ æ›´æ–°çŠ¶æ€ä¸º RUNNING
    TestRecordCRUD.update_by_uuid(db, testrecord.uuid, status=TestStatus.RUNNING)

    llm = request.session.get("llm")

    try:
        results = await run_chatflow_tests_async(
            df,
            input_uuid=testrecord.uuid,
            input_dify_url=input_dify_url,
            input_dify_api_key=input_dify_api_key,
            input_dify_username=input_username,
            concurrency=input_concurrency,
            input_judge_prompt=input_judge_prompt,
            llm=llm,
            db=db,
        )
    except asyncio.CancelledError:
        TestRecordCRUD.update_by_uuid(db, testrecord.uuid, status=TestStatus.CANCELLED)
        logger.warning(f"ä»»åŠ¡ {testrecord.uuid} è¢«å–æ¶ˆ")
        return {"cancelled": True}
    finally:
        # 5ï¸âƒ£ æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if tmp_path.exists():
                tmp_path.unlink()
                logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    # 6ï¸âƒ£ è®¡ç®—ç»“æœ
    avg_time = sum(ele.get("time_consumption") for ele in results) / len(results)
    total_time = sum(ele.get("time_consumption") for ele in results)
    avg_token = sum(ele.get("token_num") for ele in results) / len(results)
    avg_TPS = sum(ele.get("TPS") for ele in results) / len(results)
    avg_score = sum(float(ele.get("score")) for ele in results) / len(results)

    result_dict = {
        "avg_time_consumption": avg_time,
        "avg_token_num": avg_token,
        "avg_TPS": avg_TPS,
        "avg_score": avg_score,
    }

    # 7ï¸âƒ£ æ›´æ–°æ•°æ®åº“çŠ¶æ€
    TestRecordCRUD.update_by_uuid(
        db,
        testrecord.uuid,
        status=TestStatus.EXPERIMENT if mode == "experiment" else TestStatus.SUCCESS,
        duration=total_time,
        result=result_dict,
    )

    logger.success(f"âœ… Chatflow å‹æµ‹å®Œæˆ: {result_dict} (mode={mode})")
    return result_dict


async def test_workflow_non_stream_pressure_wrapper(
    testrecord,
    request,
    db,
    mode: str,
):
    """Workflow å‹æµ‹ä»»åŠ¡åŒ…è£…å™¨ï¼ˆä» TOS ä¸‹è½½æ•°æ®é›†å¹¶åœ¨å®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼‰"""

    # 1ï¸âƒ£ è¯»å–åŸºæœ¬ä»»åŠ¡å‚æ•°
    input_dify_url = testrecord.dify_api_url
    input_dify_api_key = testrecord.dify_api_key
    input_username = testrecord.dify_username
    input_concurrency = testrecord.concurrency
    input_judge_prompt = testrecord.judge_prompt
    dataset_key = testrecord.dataset_tos_key

    if not dataset_key:
        raise ValueError("âŒ å½“å‰è®°å½•ç¼ºå°‘ dataset_tos_keyï¼Œæ— æ³•ä» TOS ä¸‹è½½æ•°æ®é›†ã€‚")

    # 2ï¸âƒ£ ä¸‹è½½æ•°æ®é›†è‡³ä¸´æ—¶ç›®å½•
    tmp_dir = Path("uploads/tmp")
    tmp_dir.mkdir(parents=True, exist_ok=True)
    tmp_path = tmp_dir / f"{testrecord.uuid}_{os.path.basename(dataset_key)}"

    try:
        logger.info(f"â¬‡ï¸ æ­£åœ¨ä» TOS ä¸‹è½½æ•°æ®é›†: {dataset_key} â†’ {tmp_path}")
        await asyncio.to_thread(download_from_tos, dataset_key, tmp_path)
        logger.success(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ: {tmp_path}")
    except Exception as e:
        logger.error(f"âŒ ä» TOS ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError(f"ä¸‹è½½ TOS æ•°æ®é›†å¤±è´¥: {e}")

    # 3ï¸âƒ£ è¯»å–æ–‡ä»¶å†…å®¹
    try:
        suffix = tmp_path.suffix.lower()
        if suffix == ".csv":
            df = await asyncio.to_thread(pd.read_csv, tmp_path)
        elif suffix in [".xls", ".xlsx"]:
            df = await asyncio.to_thread(pd.read_excel, tmp_path, engine="openpyxl")
        else:
            raise ValueError(f"Unsupported file type: {suffix}")

        if mode == "experiment":
            df = df.head(3)

        logger.info(f"âœ… æ•°æ®é›† {tmp_path.name} è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œ")
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥: {e}")
        raise RuntimeError(f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {e}")

    # 4ï¸âƒ£ è·å–å‚æ•°æ¨¡æ¿å¹¶éªŒè¯è¾“å…¥
    para_df = await asyncio.to_thread(get_agent_input_para_dict, input_dify_url, input_dify_api_key)
    df = align_dify_input_types(df, para_df)
    for _, row in df.iterrows():
        row_dict = row.to_dict()
        error = validate_entry(row_dict, para_df)
        if error:
            raise ValueError(f"è¾“å…¥éªŒè¯å¤±è´¥: {error}")

    # 5ï¸âƒ£ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º RUNNING
    TestRecordCRUD.update_by_uuid(db, testrecord.uuid, status=TestStatus.RUNNING)

    llm = request.session.get("llm")

    try:
        # 6ï¸âƒ£ æ‰§è¡Œ Workflow å‹æµ‹ä»»åŠ¡
        results = await run_workflow_tests_async(
            df,
            input_uuid=testrecord.uuid,
            input_dify_url=input_dify_url,
            input_dify_api_key=input_dify_api_key,
            input_dify_username=input_username,
            concurrency=input_concurrency,
            input_judge_prompt=input_judge_prompt,
            llm=llm,
            db=db,
        )
    except asyncio.CancelledError:
        # ç”¨æˆ·ä¸»åŠ¨å–æ¶ˆ
        TestRecordCRUD.update_by_uuid(db, testrecord.uuid, status=TestStatus.CANCELLED)
        logger.warning(f"ä»»åŠ¡ {testrecord.uuid} è¢«å–æ¶ˆ")
        return {"cancelled": True}
    finally:
        # 7ï¸âƒ£ æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if tmp_path.exists():
                tmp_path.unlink()
                logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    # 8ï¸âƒ£ æ±‡æ€»è¯„æµ‹ç»“æœ
    avg_time = sum(ele.get("time_consumption") for ele in results) / len(results)
    total_time = sum(ele.get("time_consumption") for ele in results)
    avg_token = sum(ele.get("token_num") for ele in results) / len(results)
    avg_TPS = sum(ele.get("TPS") for ele in results) / len(results)
    avg_score = sum(ele.get("score") for ele in results) / len(results)

    result_dict = {
        "avg_time_consumption": avg_time,
        "avg_token_num": avg_token,
        "avg_TPS": avg_TPS,
        "avg_score": avg_score,
    }

    # 9ï¸âƒ£ æ›´æ–°ä»»åŠ¡ç»“æœçŠ¶æ€
    TestRecordCRUD.update_by_uuid(
        db,
        testrecord.uuid,
        status=TestStatus.EXPERIMENT if mode == "experiment" else TestStatus.SUCCESS,
        duration=total_time,
        result=result_dict,
    )

    logger.success(f"âœ… Workflow å‹æµ‹å®Œæˆ: {result_dict} (mode={mode})")
    return result_dict

